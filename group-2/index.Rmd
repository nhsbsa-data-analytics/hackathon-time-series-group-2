---
title: "Hackathon November 2021 - Group 2"
output: 
  html_document:
    css: ["style.css", "colours.css"]
---

```{r, echo=FALSE}
options(scipen = 999)

library(openxlsx)
library(data.table)
library(lubridate)
library(ggplot2)
library(dplyr)
library(tidyr)
library(plotly)
library(rmarkdown)
library(stringr)
library(RColorBrewer)
library(gridExtra)
library(forecast)
#library(TSA)

```

### Introduction

We have been given the task of exploring time series analysis as part of a hackathon. We have been given access to 3 datasets:

1.1 Forecasting mortality time series to estimate excess deaths in the Covid period
1.2 Exploring correlations among mortality time series in different age groups and genders
2. [Data on COVID-19 (coronavirus) by Our World in Data](https://github.com/owid/covid-19-data/tree/master/public/data)
3. 

### 1.1 Forecasting mortality time series to estimate excess deaths in the Covid period

Read in data and format as time series.

```{r}
# Read in the correct sheet as a data.table
DT <- 
  read.xlsx(
    xlsxFile = "Data/MortalityDataEngandWales_2011_2021.xlsx", 
    sheet = "TotalDeaths2011to2020"
  ) %>% 
  as.data.table()

# Name the table
setnames(DT, 1:2, c("WE","Deaths"))

# Convert WE to a date
DT[, WE := as.Date(WE, origin = as.Date("1899-12-30"))]

```

The first confirmed cases of coronavirus in the UK were on January 29 [2020], when two Chinese nationals fell ill at the Staycity Aparthotel in York.

Plot the time series.

```{r, out.width="100%"}
ggplot(DT, aes(WE, Deaths)) + geom_line() +
  scale_x_date(date_breaks = "1 year", date_labels = "%Y") +
  scale_y_continuous(labels = \(x) x/1e3) +
  geom_vline(xintercept = ymd("2020-01-29"), color="firebrick", lty=2) +
  geom_vline(xintercept = ymd("2017-01-20"), color="royalblue", lty=2) + 
  ylab("Deaths (thousands)") +
  theme(axis.title.x = element_blank())

```

Split the Covid-free period into training and testing sets in 2:1 ratio

```{r}
tr <- round(DT[WE < ymd("2020-01-29"), .N] * 2/3)
TR <- DT[1:tr]
TS <- DT[(tr+1):DT[WE < ymd("2020-01-29"), .N]]
rm(tr)

TR.ts <- ts(TR$Deaths, freq=365.25/7, start = decimal_date(ymd("2011-01-07")))
TS.ts <- ts(TS$Deaths, freq=365.25/7, start = decimal_date(ymd("2017-01-20")))
```

```{r}
m1 <- auto.arima(TR.ts)
summary(m1)
```

The auto.arima() fitted a seasonal ARIMA(1,0,1)(0,1,1). ACF/PACF plots show strong seasonality at period = 52.

```{r out.width="100%"}
ggtsdisplay(TR.ts)
```

Does differencing at lag=52 remove seasonality? ndiffs() suggests no further differencing is needed. We will stick with 1 order of seasonal differencing and 0 orders of non-seasonal differencing (same as auto.arima), but will try to find a better fitting model manually. By brute-forcing combinations of x,y,z,w in ARIMA(x,0,y)(z,1,w). The other important difference is that we will assess the fit on the unseen testing set (using RSS), in contrast to the auto.arima(), which assesses the fit on the same set that it was fitted to (using IC).

```{r out.width="100%"}
diff(TR.ts, lag=52) |> ggtsdisplay()

diff(TR.ts, lag=52) |> ndiffs()
```

```{r warning=FALSE}
R <- data.table()

max_level = 2

for (s_ma in 0:max_level) {
  
  for (s_ar in 0:max_level) {
    
    for (ns_ma in 0:max_level) {
      
      for (ns_ar in 0:max_level) {
        
        #tryCatch({
        
        m <- stats::arima(TR.ts, order = c(ns_ar,0,ns_ma), seasonal = list(order=c(s_ar,1,s_ma), period=52),method="CSS") 
        
        RSS = sum((TS[, Deaths] - forecast(TR.ts, model = m, h=158)$mean)^2)
        
        R <- rbind(R, data.table(Model = paste0("ARIMA(",ns_ar,",0,",ns_ma,")(",s_ar,",1,",s_ma,")" ),
                                   RSS = RSS))  
        rm(m, RSS)
        
        #}, error=function(e){})
        #tryCatch is used to finish the loops despite the fact that some models aren't fitted due to optim errors
        
      }
      
    }
    
  }
  
}


R[order(RSS)]
```

Among all combinations of orders up to 2, the model with the best fit in the testing set was ARIMA(0,0,1)(2,1,1). The forecast generated by it in the testing set is shown below.

```{r out.width="100%"}
m <- stats::arima(TR.ts, order = c(0,0,1), seasonal = list(order=c(2,1,1), period=52), method="CSS") 

fit <- forecast(TR.ts, model = m, h=158)

TS[, `:=`(F1 = fit$mean,
          F1_Lo = fit$lower[,2],
          F1_Hi = fit$upper[,2])]

ggplot(DT, aes(WE, Deaths)) + geom_line() +
  geom_line(aes(y=F1), data = TS, color="royalblue") + 
  geom_ribbon(aes(ymin = F1_Lo, ymax=F1_Hi), data=TS, alpha=.2) +
  scale_x_date(date_breaks = "1 year", date_labels = "%Y") +
  scale_y_continuous(labels = \(x) x/1e3) +
  geom_vline(xintercept = ymd("2020-01-29"), color="firebrick", lty=2) +
  geom_vline(xintercept = ymd("2017-01-20"), color="blue", lty=2) + 
  ylab("Deaths (thousands)") +
  theme(axis.title.x = element_blank())
```

Next, the training and testing sets are combined into one pre-Covid time series and the model identified is used to make a forecast into the Covid period.

```{r out.width="100%"}
TRTS <- ts(DT[WE < ymd("2020-01-29"), Deaths], freq=365.25/7, start = decimal_date(ymd("2011-01-07")))

m <- arima(TRTS, order = c(4,0,2), seasonal = c(0,1,0)) 

fit <- forecast(TRTS, model = m, h=89)


COV <- copy(DT[WE >= ymd("2020-01-29")])

COV[, `:=`(F2 = fit$mean,
           F2_Lo = fit$lower[,2],
           F2_Hi = fit$upper[,2])]

ggplot(DT, aes(WE, Deaths)) + geom_line() +
  #geom_line(aes(y=F1), data = TS, color="royalblue") + 
  #geom_ribbon(aes(ymin = F1_Lo, ymax=F1_Hi), data=TS, alpha=.2) +
  geom_line(aes(y=F2), data = COV, color="firebrick") + 
  geom_ribbon(aes(ymin = F2_Lo, ymax=F2_Hi), data=COV, alpha=.2) +
  scale_x_date(date_breaks = "1 year", date_labels = "%Y") +
  scale_y_continuous(labels = \(x) x/1e3) +
  geom_vline(xintercept = ymd("2020-01-29"), color="firebrick", lty=2) +
  geom_vline(xintercept = ymd("2017-01-20"), color="blue", lty=2) + 
  ylab("Deaths (thousands)") +
  theme(axis.title.x = element_blank())
```

The excess deaths are calculated as differences between the actual values and the mean forecast values.

```{r}
broman::add_commas(round(COV[, sum(Deaths-F2)]))
```



### 1.2 Exploring correlations among mortality time series in different age groups and genders

Reading and preparing the data


```{r}

DT <- data.table()

for (i in 2011:2019) {
  
  t <- read.xlsx("Data/MortalityDataEngandWales_2011_2021.xlsx", sheet = paste0("TransposedData",i), cols = c(2, 17:32)) |> as.data.table()
  
  DT <- rbind(DT, t)
  
  rm(t)
  
}

setnames(DT, 1, "WE")
DT[, WE2 := dmy(WE)]
DT[is.na(WE2), WE2 := as.Date(as.numeric(WE), origin = as.Date("1899-12-30"))]

DT[, c(1,9,10) := NULL]

setnames(DT, 15, "WE")

setnames(DT, 1:7, paste0("M", names(DT)[1:7]))
setnames(DT, 8:14, paste0("F", names(DT)[8:14]))

DT <- melt(DT, id=15)

setnames(DT, 2, "AG")

DT[, Gender := str_extract(AG, "^.")]
DT[, AG := str_remove(AG, "^.")]

DT[, Gender := factor(Gender, levels = c("M","F"))]

DT[, AG := factor(AG, levels = c("Under.1.year", "01-14", "15-44", "45-64", "65-74", "75-84", "85+"),
                                  labels = c("<01", "01-14", "15-44", "45-64", "65-74", "75-84", "85+"))]

colm <- colorRampPalette(brewer.pal(9, "Blues"))(10)[4:10]
colf <- colorRampPalette(brewer.pal(9, "OrRd"))(10)[4:10]

DT

```

```{r  fig.width=10, fig.height=5}
ggplot(DT, aes(WE, value)) +
  geom_line(aes(color=paste(Gender, AG)))  +
  facet_wrap(~Gender) +
  ylab("Deaths\n") +
  ggtitle("Raw numbers of deaths") +
  scale_color_manual("Gender/AG", values = c(colf, colm)) +
  theme(axis.title.x = element_blank())
```

Normalize numbers of deaths to facilitate comparison.

```{r fig.width=10, fig.height=5}
normalize <- function(x, na.rm = TRUE) {
  return((x- min(x)) /(max(x)-min(x)))
}

DT[, value_norm := normalize(value), by=.(Gender,AG)]

ggplot(DT, aes(WE, value_norm)) +
  geom_line(aes(color=paste(Gender, AG)))  +
  ylab("Normalized deaths\n") +
  ggtitle("Normalized numbers of deaths") +
  facet_wrap(~Gender) + 
  scale_color_manual("Gender/AG", values = c(colf, colm)) +
  theme(axis.title.x = element_blank())

```

Separate into facets.

```{r fig.width=7, fig.height=7}
ggplot(DT, aes(WE, value_norm)) +
  geom_line(aes(color=paste(Gender, AG)))  +
  ylab("Normalized deaths\n") +
  ggtitle("Normalized numbers of deaths") +
  facet_grid(AG~Gender) + 
  scale_color_manual("Gender/AG", values = c(colf, colm)) +
  theme(axis.title.x = element_blank(),
        legend.position = 'none')
```

Calculate correlation coefficients (using raw or normalized data does not make a difference to the coefficients).
Correlations are strongest amongst older age groups in both genders.

```{r}
DTm <- DT[Gender=="M"] |> dcast(WE~AG, value.var = "value_norm")


Rm <- data.table()

for (j in names(DTm)[2:8]) {
  
  for (i in names(DTm)[2:8]) {
    
    r = cor.test(DTm[, get(i)], DTm[, get(j)])$estimate
    
    Rm <- rbind(Rm, data.table(x = i, y = j, r = r))
    
  }
  
}

Rm <- Rm[x > y]

ggplot(Rm, aes(x, y, fill = r)) +
  geom_tile() +
  geom_text(aes(label = broman::myround(r,2)), color = "white", size = 4) +
  coord_fixed() +
  scale_fill_gradient(low = colm[1], high = colm[7]) +
  theme(axis.title = element_blank(),
        panel.background = element_blank()) -> gm


DTf <- DT[Gender=="F"] |> dcast(WE~AG, value.var = "value_norm")


Rf <- data.table()

for (j in names(DTf)[2:8]) {
  
  for (i in names(DTf)[2:8]) {
    
    r = cor.test(DTf[, get(i)], DTf[, get(j)])$estimate
    
    Rf <- rbind(Rf, data.table(x = i, y = j, r = r))
    
  }
  
}

Rf <- Rf[x > y]

ggplot(Rf, aes(x, y, fill = r)) +
  geom_tile() +
  geom_text(aes(label = broman::myround(r,2)), color = "white", size = 4) +
  coord_fixed() +
  scale_fill_gradient(low = colf[1], high = colf[7]) +
  theme(axis.title = element_blank(),
        panel.background = element_blank()) -> gf
```

```{r  fig.width = 10, fig.height = 5}

grid.arrange(gm, gf, nrow = 1)

```

The above correlations are at 0 lag. Let's explore the lag out of interest in one pair using the Cross-Correlation Function (CCF). 

```{r}
ccf(DTm[, `75-84`], DTm[, `85+`], main="M:75-84 vs M:85+", ylab="CCF")
```

The correlation is strongest at 0 lag. However, the strength of this correlation could be partly (even largely) due to both being driven by temporal effects (trend and/or seasonality). We can try to remove these temporal effects by removing seasonality and trend (by differencing) and pre-whitening.

```{r}
TSA::prewhiten(diff(diff(DTm[, `75-84`],52)), diff(diff(DTm[, `85+`],52)), main="M:75-84 vs M:85+", ylab="CCF")
```

The correlation at lag 0 is still strong, but weaker compared to the correlation at raw data. Presumably, this is because of other confounding effects beyond time (e.g. flu outbreaks).

If we do this on every pair of variables...

```{r message=FALSE, results=FALSE, fig.show='hide'}
Rm <- data.table()

for (j in names(DTm)[2:8]) {
  
  for (i in names(DTm)[2:8]) {
    
    ccf <- TSA::prewhiten(diff(diff(DTm[, get(i)],52)), diff(diff(DTm[, get(j)],52)))$ccf
    
    dt <- data.table(ccf = as.numeric(ccf$acf),lag = as.numeric(ccf$lag))
    
    r_0 = dt[lag==0, ccf]
    
    r_max = (TSA::prewhiten(diff(diff(DTm[, get(i)],52)), diff(diff(DTm[, get(j)],52)))$ccf$acf) |> max()
    
    Rm <- rbind(Rm, data.table(x = i, y = j, r_0 = r_0, r_max = r_max))
    
  }
  
}

ggplot(Rm, aes(x, y, fill = r_0)) +
  geom_tile() +
  geom_text(aes(label = broman::myround(r_0,2)), color = "white", size = 4) +
  coord_fixed() +
  scale_fill_gradient(low = colm[1], high = colm[7]) +
  theme(axis.title = element_blank(),
        panel.background = element_blank()) -> m1

ggplot(Rm, aes(x, y, fill = r_max)) +
  geom_tile() +
  geom_text(aes(label = broman::myround(r_max,2)), color = "white", size = 4) +
  coord_fixed() +
  scale_fill_gradient(low = colm[1], high = colm[7]) +
  theme(axis.title = element_blank(),
        panel.background = element_blank()) -> m2



Rf <- data.table()

for (j in names(DTf)[2:8]) {
  
  for (i in names(DTf)[2:8]) {
    
    ccf <- TSA::prewhiten(diff(diff(DTf[, get(i)],52)), diff(diff(DTf[, get(j)],52)))$ccf
    
    dt <- data.table(ccf = as.numeric(ccf$acf),lag = as.numeric(ccf$lag))
    
    r_0 = dt[lag==0, ccf]
    
    r_max = (TSA::prewhiten(diff(diff(DTf[, get(i)],52)), diff(diff(DTf[, get(j)],52)))$ccf$acf) |> max()
    
    Rf <- rbind(Rf, data.table(x = i, y = j, r_0 = r_0, r_max = r_max))
    
  }
  
}

ggplot(Rf, aes(x, y, fill = r_0)) +
  geom_tile() +
  geom_text(aes(label = broman::myround(r_0,2)), color = "white", size = 4) +
  coord_fixed() +
  scale_fill_gradient(low = colf[1], high = colf[7]) +
  theme(axis.title = element_blank(),
        panel.background = element_blank()) -> f1

ggplot(Rf, aes(x, y, fill = r_max)) +
  geom_tile() +
  geom_text(aes(label = broman::myround(r_max,2)), color = "white", size = 4) +
  coord_fixed() +
  scale_fill_gradient(low = colf[1], high = colf[7]) +
  theme(axis.title = element_blank(),
        panel.background = element_blank()) -> f2

```

```{r fig.width = 12, fig.height = 8}
grid.arrange(m1, m2, f1, f2, nrow = 2)
```

A few things to note

* There are still fairly strong correlations among older age groups, but weaker compared to before differencing and pre-whitening.

* Coefficient matrices are not symmetrical: this is because pre-whitening function fits a TS model to the first series and then uses it to pre-whiten both series. So the order of series in a pair matters, but in this case the differences are tiny.

* The left sides show correlation coefficients at 0 lag, while the right sides show the maximum correlation coefficient at any lag. (Right coefficient can be larger than left, but not vice versa.) Here, it only makes a small difference in low-strength correlation pairs, which makes it negligible/unimportant.


###### 2. Data on COVID-19 (coronavirus) by Our World in Data

This data can be read in directly from GitHub using `{readr}`. The `col_types` have to be specified up front otherwise it doesn't load correctly.

```{r}
# Read in COVID data from GitHub with correct types
covid_df <- readr::read_csv(
  file = "https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/owid-covid-data.csv",
  col_types = "cccDdddddddddddddddddddddddddddddcddddddddddddddddddddddddddddddddd"
)

# Inspect it
paged_table(covid_df)
```

There are many columns and groups but we are going to focus on United Kingdom with a subset of columns.

```{r}
# Filter and subset columns
uk_covid_df <- covid_df %>% 
  filter(location == "United Kingdom") %>%
  # Rename some columns so they are either {col} or {col}_cumulative
  select(
    date, 
    cases = new_cases,
    cases_cumulative = total_cases,
    deaths = new_deaths, 
    deaths_cumulative = total_deaths, 
    #excess_mortality, # this is the % difference 
    excess_mortality_cumulative = excess_mortality_cumulative_absolute)

# Inspect it
paged_table(uk_covid_df)
```

We can see that the `excess_mortality_cumulative` is only updated weekly. So we forward `fill`.

```{r}
# Replace missing data from top to bottom
uk_covid_df <- uk_covid_df  %>%
  fill(excess_mortality_cumulative)

# Check it's worked
paged_table(uk_covid_df)
```

We also reverse engineer an `excess_mortality` column.

```{r}
# Reverse engineer the cumulative sum 
uk_covid_df <- uk_covid_df %>%
  # Replace all NAs with 0
  replace(is.na(.), 0) %>%
  mutate(
    excess_mortalilty = 
      excess_mortality_cumulative - lag(excess_mortality_cumulative)
  ) %>%
  # Catch the first NA that has become 0 because of the lag calc
  replace(is.na(.), 0) %>%
  dplyr::relocate(excess_mortalilty, .before = excess_mortality_cumulative)

# Inspect
paged_table(uk_covid_df)
```

Check the date range.

```{r,collapse=TRUE}
# Print off a summary of the dates
print(paste("Start date of cases:",uk_covid_df %>% summarise(min(date)) %>% pull()))
print(paste("Start date of deaths:", uk_covid_df %>% filter(deaths_cumulative > 0) %>% summarise(min(date)) %>% pull()))
print(paste("End date of cases:",uk_covid_df %>% summarise(max(date)) %>% pull()))
print(paste("End date of deaths:", uk_covid_df %>% filter(deaths_cumulative > 0) %>% summarise(max(date)) %>% pull()))
```

Quickly plot some of the cols (`{col}` on the right and `{col}_cumulative` on the right).

```{r, out.width="100%"}
# Chart the cols (not sure how you can have negative deaths)
p <- uk_covid_df %>% 
  pivot_longer(cols = -date, names_to = "measure", values_to = "count") %>%
  mutate(measure = as.factor(measure)) %>%
  ggplot(aes(x = date, y = count)) +
  geom_line() +
  facet_wrap(facets = ~measure, ncol = 2, scales = "free_y", shrink = FALSE) + 
  theme_minimal()

ggplotly(p)
```
